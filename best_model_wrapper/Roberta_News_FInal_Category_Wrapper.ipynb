{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83xD9vTJi70h"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KOvjwrvji2LW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install transformers\n",
        "! pip install hazm\n",
        "from hazm import *\n",
        "import copy\n",
        "import transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, recall_score, roc_auc_score, precision_score\n",
        "import math\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "import json\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import codecs\n",
        "from shutil import copyfile\n",
        "random.seed(12345)\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "# downloading persian stopwords\n",
        "# ######removed\n",
        "\n",
        "# Preprocessing class\n",
        "class Preprocessing:\n",
        "\n",
        "  @staticmethod\n",
        "  def remove_punctuations(text):\n",
        "    new_text = []\n",
        "    for l in text:\n",
        "      if l not in string.punctuation + '\\u00AB' + '\\u00BB' + '\\u060C' + '\\u061B' + '\\u061F':\n",
        "        new_text.append(l)\n",
        "      else:\n",
        "        new_text.append(' ')\n",
        "    return ''.join(new_text)\n",
        "  \n",
        "  @staticmethod\n",
        "  def remove_numbers(text):\n",
        "    new_text = []\n",
        "    for l in text:\n",
        "      if l not in '0123456789۰۱۲۳۴۵۶۷۸۹':\n",
        "        new_text.append(l)\n",
        "      else:\n",
        "        new_text.append(' ')\n",
        "    return ''.join(new_text)\n",
        "\n",
        "  @staticmethod\n",
        "  def remove_stopwords(text):\n",
        "    normalizer = Normalizer()\n",
        "    stopwords = [normalizer.normalize(x.strip()) for x in codecs.open('stopwords.txt','r','utf-8').readlines()]\n",
        "    tokens = word_tokenize(text)\n",
        "    new_text = []\n",
        "    for token in tokens:\n",
        "      if token not in stopwords:\n",
        "        new_text.append(token)\n",
        "      else:\n",
        "        new_text.append(' ')\n",
        "    return ' '.join(new_text)\n",
        "\n",
        "  @staticmethod\n",
        "  def remove_extra_space(text):\n",
        "    new_text = re.sub(r'\\s+',' ',text)\n",
        "    return new_text\n",
        "\n",
        "# a class to hold our data structure\n",
        "class Data:\n",
        "  def __init__(self, data):\n",
        "    self.text = Preprocessing.remove_extra_space(Preprocessing.remove_stopwords(Preprocessing.remove_numbers(Preprocessing.remove_punctuations(data['text']))))\n",
        "    self.category = data['category']\n",
        "\n",
        "# loading pars roberta and tokenizer\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n",
        "# v3.0\n",
        "model_name_or_path = \"HooshvareLab/roberta-fa-zwnj-base\"\n",
        "config = AutoConfig.from_pretrained(model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast='True')\n",
        "# model = TFAutoModel.from_pretrained(model_name_or_path)  For TF\n",
        "parsbert = AutoModel.from_pretrained(model_name_or_path)\n",
        "\n",
        "\n",
        "# defining our transformer model\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "  def __init__(self, roberta):\n",
        "    super(TransformerModel, self).__init__()\n",
        "    self.roberta = roberta\n",
        "    # we only use one linear head on the parsbert\n",
        "    self.linear_head = nn.Linear(768, len(label_encoder.classes_))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # main task\n",
        "    x = self.roberta(x['input_ids'],x['attention_mask'])\n",
        "    logits = self.linear_head(x.pooler_output)\n",
        "    return logits\n",
        "\n",
        "# load model\n",
        "## removed\n",
        "! pip install -U --no-cache-dir gdown --pre\n",
        "## removed\n",
        "model = torch.load('project_roberta_final_category.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl2En3UKZfqh"
      },
      "source": [
        "# Test The Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bRFgyTB2ZiK5"
      },
      "outputs": [],
      "source": [
        "def test(data):\n",
        "  data = Data(data)\n",
        "  device = 'cuda:0'\n",
        "  func_model = model.to(device)\n",
        "  text_tokens = tokenizer.encode_plus(\n",
        "    str(data.category),\n",
        "    str(data.text),\n",
        "    add_special_tokens=True,\n",
        "    max_length=512,\n",
        "    pad_to_max_length=True,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True)\n",
        "  input_ids = text_tokens[\"input_ids\"].view(1, -1).to(device)\n",
        "  attention_mask = text_tokens[\"attention_mask\"].view(1, -1).to(device)\n",
        "  feed_dict = {\n",
        "    'input_ids': input_ids,\n",
        "    'attention_mask': attention_mask}\n",
        "  output = func_model(feed_dict)\n",
        "  pred = output.argmax(dim=1, keepdim=True)\n",
        "  if pred.item() == 1:\n",
        "    return 'مهم'\n",
        "  else:\n",
        "    return 'غیرمهم'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "SZ3ljDgdzn7P",
        "outputId": "260a2e3d-e3b9-4132-fa19-34b16acc1778"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'مهم'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test\n",
        "test()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Roberta_News_FInal_Category_Wrapper.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
